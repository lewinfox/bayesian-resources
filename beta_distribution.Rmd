---
title: "The beta distribution"
theme: cosmo
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: yes
---

Most of the examples here come from [countbayesie.com](https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics).

# The beta distribution and beta function
The beta distribution is characterised by two shape parameters, $\alpha$ and $\beta$. The page linked above uses the example of email open rates to illustrate this, so I will use that example here. In that context $\alpha$ is the number of recipients who opened an email, and $\beta$ is the number who did not. More generally $\alpha$ is the number of observed positive classes and $\beta$ is the number of "not-positive" classes.

The value of a beta distribution with parameters $\alpha$ and $\beta$ at point $x$ is given by:

$$f_{beta}(x, \alpha, \beta) = \frac{x^{\alpha - 1} {(1 - x)^{\beta - 1}}} {Beta(\alpha, \beta)}$$

Where $Beta(\alpha, \beta)$ is a normalising constant calculated by the beta function:

**TODO:** Why is it important to normalise?

$$Beta(\alpha, \beta) = \frac{(\alpha - 1)! (\beta - 1)!}{(\alpha + \beta - 1)!}$$

R has builtins for these (see `?base::beta` and `?stats::Beta`), but let's write our own anyway.

**NOTE:** These fail for $\alpha$ and $\beta$ over about 70 - R's builtins do not.
```{r}
beta_fn <- function(alpha, beta) {
  (factorial(alpha - 1) * factorial(beta - 1)) / factorial(alpha + beta - 1)
}

beta_dist <- function(x, alpha, beta) {
  (x^(alpha - 1) * (1 - x)^(beta - 1)) / beta_fn(alpha, beta)
}
```

I'm also going to write a convenience function to plot a beta distribution between 0 and 1 given $\alpha$ and $\beta$.

```{r}
plot_beta_distribution <- function(alpha, beta) {
  x <- seq(0, 1, 0.01)
  plot(
    x,
    beta_dist(x, alpha, beta),
    type = "l",
    main = paste0("Beta distribution (", alpha, ",", beta, ")"),
    xlab = "x",
    ylab = "beta_dist(x)"
  )
  invisible(TRUE)
}
```

## Some beta distributions

A beta distribution with $\alpha = 1$ and $\beta = 1$ is a uniform distribution with a value of 1.

**TODO:** Can you have negative a or b?

**TODO:** What are the implications of this (1, 1) distribution? If I have two observations of which one is positive, are we saying that the beta distribution contains no information? Is that even a meaningful question?
```{r}
plot_beta_distribution(1, 1)
```

Increasing (but equal) values of $\alpha$ and $\beta$ produce successively narrower distributions centred on 0.5.
```{r}
plot_beta_distribution(2, 2)
```

```{r}
plot_beta_distribution(5, 5)
```

```{r}
plot_beta_distribution(20, 20)
```

If $\alpha$ and $\beta$ are not equal then more interesting things happen - the distribution shifts left or right and becomes asymmetric.
```{r}
plot_beta_distribution(5, 10)
```

The absolute value of $\alpha$ and $\beta$ affects the shape too - compare `plot_beta_distribution(5, 10)` (above) with `plot_beta_distribution(20, 40)` (below). Larger values of $\alpha$ and $\beta$ make the distribution narrower and higher (i.e. uncertainty is reduced).
```{r}
plot_beta_distribution(20, 40)
```

This is where things get more interesting.

# Example: Checking if a coin is fair
Say we have a coin and we want to test if it's fair, i.e. that the probability of it coming up heads, $P(heads)$, is 0.5. Ideally we would toss the coin an infinite number of times, in which case we would expect to find an exactly equal number of heads and tails. In reality we can't do this, and we will probably end up with differing numbers of heads and tails. 

The beta distribution allows us to make statements about how likely it is that the coin is fair based on a finite number of observations.

We flip the coin 30 times and observe 11 heads. At this point we might say that the coin is not fair, because $P(heads)$ appears to be 11/30, not 15/30. On the other hand, 30 flips is not very many so it's not unreasonable that a fair coin would turn up only 11 heads.

Assuming heads is what we're interested in, heads becomes alpha and tails becomes beta. Our distribution look like this:

```{r}
plot_beta_distribution(11, 19)
```

**TODO:** I'm using "chance" here - what's the correct term?

What's happening here is we are saying "for every possible value of $P(heads)$, what is the probability density based on our observed data?". We can see that the chance of $P(heads)$ being 0 or 1 is zero, which makes sense because we observed both heads and tails in the data so it can't be either of these extremes. In general it looks like the chance of it being outside the range 0.2 - 0.6 is extremely low.

The distribution peaks just below 0.4. In fact, we can double check the exact value:

```{r}
p_heads <- seq(0, 1, 0.01)
beta_x <- beta_dist(p_heads, 11, 19)
beta_x[beta_x == max(beta_x)]
```

So $P(heads)$ is 0.36, and our coin is biased?

**TODO:** Not sure about this bit - is the terminology correct?

Not so fast. We're working with a *distribution*, so making absolute statements is risky. Technically this is a probability density function, so the probability (/likelihood?) of $P(heads)$ taking any particular value is 0. We really want to say "what is the probability of $P(heads)$ falling between values A and B?".

This is the same as saying "what is the integral of the function between A and B". An easy way to calculate this is by converting the function to a cumulative distribution. R has a built in function for this, `pbeta`, which gives you the quantile of the requested value based on the shape parameters you provide.

We can plot the cumulative distribution using `pbeta` (you could also look into `?ecdf`):

```{r}
x <- seq(0, 1, 0.01)
p <- pbeta(x, 11, 19)
plot(x, p, main = "Cumulative distribution for beta(11, 19)", type = "l")
```

Our analysis earlier suggested that $P(heads)$ is about 0.36, so let's calculate the probablity it lies between 0.3 and 0.4.

```{r}
pbeta(0.4, 11, 19) - pbeta(0.3, 11, 19)
```

Based on this data we think there's a 42% chance that $P(heads)$ is between 0.3 and 0.4.

We will also want to compute confidence intervals - R also provides a `qbeta` function for calcualting quantiles. E.g. for the 95% interval (from p = 0.025 to p = 0.975):

```{r}
p_min <- qbeta(0.025, 11, 19)
p_max <- qbeta(0.975, 11, 19)
cat(paste("There is a 95% chance that P(heads) is between", p_min, "and", p_max, "\n"))
```

That's pretty broad! Now let's have a look at how we can incorporate priors into the estimate.

# Incorporating priors
In our coin example we probably think it's unlikely that $P(heads)$ is all that far away from 0.5. We can incorporate those beliefs using priors. Priors take the form of another beta distribution where we have set $\alpha$ and $\beta$ ourselves based on previous experiments or intuition.

We can add a prior distribution $d_{prior} = f_{beta}(\alpha_{prior}, \beta_{prior})$ to our observed distribution $d_{obs} = f_{beta}(\alpha_{obs}, \beta_{obs})$ by just adding together the values for $\alpha$ and $\beta$:

$$d_{final} = f_{beta}(\alpha_{obs} + \alpha_{prior}, \beta_{obs} + \beta_{prior})$$
If we assume the coin is fair then we expect the number of heads to be equal to the number of tails, meaning that $\alpha$ = $\beta$. We saw earlier that the absolute value of the parameters affects the distribution so using (2, 2) for our priors is not the same as using (3, 3). 

As a recap, this plot shows the observed distribution and the priors we will add.

```{r, collapse=TRUE}
# Generate data
p <- seq(0, 1, 0.01)
observed_distribution <- beta_dist(p, 11, 19)
weak_prior <- beta_dist(p, 1, 1)
medium_prior <- beta_dist(p, 5, 5)
strong_prior <- beta_dist(p, 20, 20)

# Plot
plot(p, observed_distribution, type = "l", col = "black", ylim = c(0, 7))
lines(p, weak_prior, type = "l", col = "red")
lines(p, medium_prior, type = "l", col = "blue")
lines(p, strong_prior, type = "l", col = "green")
legend(
  "topright",
  legend = c("observed", "weak prior", "medium prior", "strong prior"),
  col = c("black", "red", "blue", "green"),
  lty = 1
  )
```

Now the same, but showing the distributions that result from adding the priors to the observations:
```{r, collapse=TRUE}
# Generate data
p <- seq(0, 1, 0.01)
observed_distribution <- beta_dist(p, 11, 19)
weak_prior <- beta_dist(p, 11 + 1, 19 + 1)  # Adding alpha and beta together
medium_prior <- beta_dist(p, 11 + 5, 19 + 5)
strong_prior <- beta_dist(p, 11 + 20, 19 + 20)

# Plot
plot(p, observed_distribution, type = "l", col = "black", ylim = c(0, 7))
lines(p, weak_prior, type = "l", col = "red")
lines(p, medium_prior, type = "l", col = "blue")
lines(p, strong_prior, type = "l", col = "green")
legend(
  "topright",
  legend = c("observed", "weak prior", "medium prior", "strong prior"),
  col = c("black", "red", "blue", "green"),
  lty = 1
  )
```

Note how the increasingly strong priors move the distribution towards 0.5 and tighten it.

# Bayesian A/B testing
Now that we've seen how beta distributions are constructed let's look at a more realistic example. We sent two variants of an email, A and B, and we want to see if there is a difference in the open rate. We email 300 people with half getting version A and half getting version B. The reuslts are below:

Email | Number sent | Number opened ($\alpha$) | Number unopened ($\beta$)
-|-|-|-
A | 150 | 36 | 114
B | 150 | 50 | 100

On the face of it we would say that B has done better than A. But how confident are we of that result? Let's have a look at the distributions:

```{r, collapse=TRUE}
# Generate data
p <- seq(0, 1, 0.01)
beta_y <- beta_dist(p, 36, 114)

# Plot
plot(p, beta_y, main = "Beta distributions for emails A and B",
     type = "l", ylab = "beta_dist(p)")
lines(seq(0, 1, 0.01), beta_dist(seq(0, 1, 0.01), 50, 100), type = "l", col = "red")
legend(
  "topright",
  legend = c("A", "B"),
  col = c("black", "red"),
  lty = 1
  )
```

We also want to incoporate some priors - maybe we know from previous campaigns that the typical open rate for this type of email is 30%. Because we aren't very confident about the vaue we will use weak priors of 3 and 7, meaning that our best guess at open rates looks like this:

```{r}
plot_beta_distribution(3, 7)
```

We can incorporate that as we've seen before:

```{r, collapse=TRUE}
p <- seq(0, 1, 0.01)
beta_y <- beta_dist(p, 36 + 3, 114 + 7)
plot(p, beta_y, main = "Beta distributions for emails A and B with priors",
     type = "l", ylab = "beta_dist(p)")
lines(seq(0, 1, 0.01), beta_dist(seq(0, 1, 0.01), 50 + 3, 100 + 7), type = "l", col = "red")
legend(
  "topright",
  legend = c("A", "B"),
  col = c("black", "red"),
  lty = 1
  )
```

There's a fair bit of overlap between the distributions. What if we were just unlucky with A (or lucky with B)? How can we be confident of our results?

## Monte Carlo simulation
Given that we have these distributions, we can randomly sample from them and see in how many cases B's open rate actually exceeds A's, and by how much. **Assuming our distributions are reasonable** this should give a good picture of the relative merits of the two emails.

We will take 1 million random samples from each distribution. The `rbeta` function randomly samples from a specified beta distribution.

```{r}
n_samples <- 1e6
a_samples <- rbeta(n_samples, 36 + 3, 114 + 7)
b_samples <- rbeta(n_samples, 50 + 3, 100 + 7)
```

`a_samples` and `b_samples` now contain the simulated open rates for a million A/B tests! We can look at each pair of elements and see how frequently B's open rate exceeded A's

```{r}
probability_b_is_better <- sum(b_samples > a_samples) / n_samples
cat(paste("B's open rate was higher", probability_b_is_better * 100, "% of the time"))
```

OK, so we can be pretty confident that B did better. But *how much better*? For that we can look at the difference in open rate in our simulated experiments.

```{r}
open_rate_uplift <- (b_samples / a_samples) - 1
hist(open_rate_uplift, main = "Open rate of B relative to A",
     xlab = "Open rate uplift factor")
```

By eye it looks like B's open rate tends to be around 20% higher than A's. We can be more scientific about this by using a cumulative distribution function. If you're not familiar with the `ecdf` function, try `?ecdf`.

```{r}
open_rate_ecdf <- ecdf(open_rate_uplift)
uplift <- seq(-1, 2, 0.01)
plot(uplift, open_rate_ecdf(uplift), type = "l", xlab = "Open rate uplift",
     ylab = "proportion of trials",
     main = "Cumulative distribution of open rate uplift of B over A")
```

We can now use `open_rate_ecdf` to do things like look at the median open rate uplift:
```{r}
cat(paste("The median uplift in open rates was", open_rate_ecdf(0.5) * 100, "%"))
```

or calculate the 95% confidence interval for uplift;
```{r}
lower_limit <- open_rate_ecdf(0.025) * 100
upper_limit <- open_rate_ecdf(0.975) * 100
cat(paste("The 95% confidence interval for the open rate uplift is between",
          lower_limit, "% and", upper_limit, "%"))
```
